data(state)
statedata = data.frame(state.x77)
str(statedata)
summary(statedata)
head(statedata)
LMModel = lm(Life.Exp ~ ., data=statedata)
summary(LMModel)
LMPredict = predict(LMModel, data=statedata)
sum((LMPredict - statedata$Life.Exp)^2)
LMModel2 = lm(Life.Exp ~ Population + Murder + Frost + HS.Grad, data=statedata)
summary(LMModel2)
LMPredict2 = predict(LMModel2, data=statedata)
sum((LMPredict2 - statedata$Life.Exp)^2)
cor(statedata)
library(rpart)
library(rpart.plot)
TreeModel = rpart(Life.Exp ~ ., data=statedata)
summary(TreeModel)
prp(TreeModel)
TreePredict = predict(TreeModel, data=statedata)
sum((TreePredict - statedata$Life.Exp)^2)
TreeModel1 = rpart(Life.Exp ~ ., data=statedata, minbucket = 5)
prp(TreeModel1)
TreePredict1 = predict(TreeModel1, data=statedata)
sum((TreePredict1 - statedata$Life.Exp)^2)
TreeModel2 = rpart(Life.Exp ~ Area, data=statedata, minbucket = 1)
prp(TreeModel2)
TreePredict2 = predict(TreeModel2, data=statedata)
sum((TreePredict2 - statedata$Life.Exp)^2)
library(caret)
library(e1071)
set.seed(111)
tr.control = trainControl(method = "cv", number = 10)
cp.grid = expand.grid(.cp = (0:10)*0.001)
(0:50)*0.01
(0:50)*0.01
cp.grid = expand.grid(.cp = (1:50)*0.01)
tr = train(Life.Exp ~ ., data = statedata, method = "rpart", trControl = tr.control, tuneGrid = cp.grid)
library(caret)
install.packages("caret")
install.packages("e1071")
library(caret)
library(e1071)
set.seed(111)
tr.control = trainControl(method = "cv", number = 10)
cp.grid = expand.grid(.cp = (1:50)*0.01)
tr = train(Life.Exp ~ ., data = statedata, method = "rpart", trControl = tr.control, tuneGrid = cp.grid)
cp.grid = expand.grid(.cp = (0:50)*0.01)
tr = train(Life.Exp ~ ., data = statedata, method = "rpart", trControl = tr.control, tuneGrid = cp.grid)
tr
BestTree = rpart(Life.Exp ~ ., data = statedata, cp=0.11)
prp(BestTree)
BestTree = rpart(Life.Exp ~ ., data = statedata, cp=0.12)
prp(BestTree)
prp(BestTree)
BestTree = rpart(Life.Exp ~ ., data = statedata, minbucket=5, cp=0.12)
prp(BestTree)
BestTree = rpart(Life.Exp ~ ., data = statedata, cp=0.12)
BestTree = rpart(Life.Exp ~ ., data = statedata, cp=0.11)
prp(BestTree)
prp(BestTree)
PredictTree = predict(BestTree,data=statedata)
sum((PredictTree - statedata$Life.Exp)^2)
sum((TreePredict2 - statedata$Life.Exp)^2)
sum((TreePredict1 - statedata$Life.Exp)^2)
set.seed(111)
tr.control = trainControl(method = "cv", number = 10)
cp.grid = expand.grid(.cp = (0:50)*0.01)
tr = train(Life.Exp ~ Area, data = statedata, method = "rpart", trControl = tr.control, tuneGrid = cp.grid)
tr
BestTree2 = rpart(Life.Exp ~ Area, data = statedata, cp=0.01)
prp(BestTree2)
51e+3
PredictTree2 = predict(BestTree2,data=statedata)
sum((PredictTree2 - statedata$Life.Exp)^2)
prp(BestTree)
setwd("~/Pravin/MITAnalyticsEdge/MITAnalyticsEdge/Week 4")
gerber = read.csv("gerber.csv")
rm(list=ls())
gerber = read.csv("gerber.csv")
str(gerber)
table(gerber$voting)
108696/(108696+235388)
table(gerber$voting, gerber$control)
civic = subset(gerber, gerber$civicduty ==1)
hawthorne = subset(gerber, gerber$hawthorne ==1)
self = subset(gerber, gerber$self ==1)
neighbors = subset(gerber, gerber$neighbors ==1)
table(civic$voting)
12021/(12021+26197)
table(hawthorne$voting)
12316/(12316+25888)
table(neighbors$voting)
14438/(14438+23763)
table(self$voting)
13191/(13191+25027)
tapply(gerber$voting, gerber$civicduty, mean)
tapply(gerber$voting, gerber$neighbors, mean)
LogModel = glm(voting ~ civicduty + hawthorne + self + neighbors, data=gerber, family=binomial)
summary(LogModel)
LogPredict = predict(LogModel, data=gerber, type="response")
LogPredict = predict(LogModel, data=gerber, type="response")
table(gerber$voting, LogPredict > 0.3)
(51966+134513)/(51966+134513+100875+56730)
table(gerber$voting, LogPredict > 0.5)
235388/(235388+108696)
table(gerber$voting)
108696/(108696+235388)
table(gerber$voting)
235388/(108696+235388)
235388/(235388+108696)
(51966+134513)/(51966+134513+100875+56730)
library(ROCR)
ROCRpred = prediction(LogPredict, gerber$voting)
as.numeric(performance(ROCRpred,"auc")@y.values)
library(rpart)
library(rpart.plot)
TreeModel = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber)
CARTTreeModel = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber)
rm(TreeModel)
CARTTreeModel = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber,type = "class")
CARTTreeModel = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber,method = "class")
RegTreeModel = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber)
prp(RegTreeModel)
prp(CARTTreeModel)
prp(RegTreeModel)
RegTreeemodel2 = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber, cp=0.0)
prp(RegTreeemodel2)
str(gerber)
RegTreeemodel3 = rpart(voting ~ civicduty + hawthorne + self + neighbors + sex, data=gerber, cp=0.0)
prp(RegTreeemodel3)
RegTreeemodel4 = rpart(voting ~ control, cp=0.0)
prp(RegTreeemodel4)
RegTreeemodel4 = rpart(voting ~ control, data = gerber, cp=0.0)
prp(RegTreeemodel4)
RegTreeemodel5 = rpart(voting ~ control + sex, data=gerber, cp=0.0)
prp(RegTreeemodel5)
prp(RegTreeemodel4)
prp(RegTreeemodel4, digits = 6)
abs(0.296638-0.34)
RegTreeemodel5 = rpart(voting ~ control + sex, data=gerber, cp=0.0)
prp(RegTreeemodel5)
prp(RegTreeemodel5,digits=6)
abs(0.290456-0.334176)
abs(0.302795-0.345818)
(abs(0.290456-0.334176)) - (abs(0.302795-0.345818))
LogModel2 = glm(voting ~ sex + control, data=gerber, family=binomial)
summary(LogModel2)
Possibilities = data.frame(sex=c(0,0,1,1),control=c(0,1,0,1))
Possibilities
predict(LogModelSex, newdata=Possibilities, type="response")
predict(LogModel2, newdata=Possibilities, type="response")
predict(LogModel2, newdata=Possibilities, type="response")
abs(0.302795-0.2908065)
abs(0.290456-0.2908065)
LogModel3 = glm(voting ~ sex + control + sex:control, data=gerber, family="binomial")
summary(LogModel3)
predict(LogModel3, newdata=Possibilities, type="response")
abs(0.290456-0.2904558)
RegTreeemodel2 = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber, cp=0.0)
prp(RegTreeemodel2)
rm(ls = list())
rm(list = ls())
census = read.csv("census.csv")
census = read.csv("census.csv")
library(caTools)
set.seed(2000)
split = sample.split(census$over50k, SplitRatio = 0.6)
train = subset(census, split == T)
test = subset(census, split == F)
LogModel = glm(over50k ~ ., data = train, family = binomial)
summary(LogModel)
# Logisitic regression model
PredictLog = predict(LogModel, newdata = test, type = "response")
PredictLog
table(test$over50k, PredictLog > 0.5)
(9051 + 1888)/nrow(test) # Accuracy = 0.8552107
# Baseline model for test set
table(test$over50k)
9713/nrow(test) # Accuracy = 0.7593621
# CART Model
library(rpart)
library(rpart.plot)
CARTModel = rpart(over50k ~ ., data = train, method = "class")
prp(CARTModel)
PredictCART = predict(CARTModel, newdata = test, type = "class")
table(test$over50k, PredictCART)
(9243+1596/nrow(test))
(9243+1596)/nrow(test)
# Calculate AUC for the CART Model
library(ROCR)
PredictROC = predict(CARTModel, newdata = test)
pred = prediction(PredictROC[,2],test$over50k)
perf = performance(pred,"tpr","fpr")
plot(perf)
as.numeric(performance(pred, "auc")@y.values)
prp(CARTModel)
# AUC for LogModel
PredictROC1 = predict(LogModel, newdata = test)
PredictROC1
pred1 = prediuction(PredictROC1, test$over50k)
pred1 = prediction(PredictROC1, test$over50k)
perf1 = performance(pred1, "tpr", "fpr")
plot(perf1)
as.numeric(performance(pred1, "auc")@y.values)
as.numeric(performance(pred, "auc")@y.values)
plot(perf)
prp(CARTModel)
plot(perf)
# Random FOrest Model
library(randomForest)
set.seed(1)
trainSmall = train[sample(nrow(train), 2000), ]
set.seed(1)
ForestModel = randomForest(over50k ~ ., data = trainSmall)
ForestPredict = predict(ForestModel, newdata = test)
table(test$over50k, ForestPredict)
(8843+2049)/nrow(test)
(8843+2049)/nrow(test) # Accuracy = 0.8515362
vu = varUsed(ForestModel, count=TRUE)
vusorted = sort(vu, decreasing = FALSE, index.return = TRUE)
dotchart(vusorted$x, names(ForestModel$forest$xlevels[vusorted$ix]))
varImpPlot(ForestModel)
# Cross validation for the CART Model
set.seed(2)
# Cross validation for the CART Model
library(caret)
library(e1071)
set.seed(2)
tr.control = trainControl(method = "cv", number = 10)
tr = train(over50k ~ ., data = train, method = "rpart", trControl = tr.control, tuneGrid = cp.grid)
set.seed(2)
tr.control = trainControl(method = "cv", number = 10)
cp.grid = expand.grid( .cp = seq(0.002,0.1,0.002))
tr = train(over50k ~ ., data = train, method = "rpart", trControl = tr.control, tuneGrid = cp.grid)
tr
tr
CARTModel2 = rpart(over50k ~ ., data = train, method = "class", cp = 0.002)
CARTPredict2 = predict(CARTModel2, newdata = test, type = "class")
table(test$over50k, CARTPredict2)
(9178+1838)/nrow(test) # Accuracy =
prp(CARTModel2)
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("dplyr")
install.packages("dplyr")
library(ggplot2)
library(dplyr)
diamonds
rstats::diamonds
str(diamonds)
summary(diamonds)
mean(diamonds$price) %>% round()
diamonds$price %>% mean()
install.packages("magrittr")
install.packages("magrittr")
library(magrittr)
diamonds$price %>% mean()
install.packages("devtools")
devtools::install_github("rstudio/EDAWR")
?storms
library(EDAWR)
storms
devtools::install_github("rstudio/EDAWR")
devtools::install_github("rstudio/EDAWR")
