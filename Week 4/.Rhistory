# Vector ----
c(1,2,3,4,5,6,7,8,9,10)
# Vector ----
myVector = c(1,2,3,4,5,6,7,8,9,10)
myVector
myVector[1]
myVector2 = c(1:10)
myVector + myVector2
myVector2 = c(1:15)
myVector + myVector2
newVector = ("A","B","C")
newVector2 = ("D","E","F")
newVector = c("A","B","C")
newVector2 = c("D","E","F")
newVector + newVector2
# list ----
list(1,2,3,"Okay")
# list ----
list(1,2,3,"Okay",c(1:10))
myVector = c(1:10)
myVec2 = myVector
myVector2 = myVector
rm(myVec2)
myVector = c(1,2,3,4,5,6,7,8,9,10)
myVector[c(2:5)]
myVector[-9]
myVector[-9,-10]
myVector[c(-9,-10)]
myValues = c(1:99)
myValues = c(1:100)
myValues =
1:100
myValues =  1:100
myValues
mean(myValues)
mean(myValues)
mean(c(1:10))
median(myValues)
min(myValues)
max(myValues)
sum(myValues)
sd(myValues)
class(myValues)
class(myVector)
length(myValues)
log(myValues)
rep(1:8)
c(1:8)
log(myValues)
log10(myValues)
log2(myValues)
log10(980000)
log10(780000)
log10(180000)
log10(1000001)
log10(1000010)
log10(1010010)
log10(5010010)
mySqrt = sqrt(myValues)
mySqrt
?rnorm
rnorm(100)
rnorm(100,9000,89)
hist(rnorm(100,9000,89))
hist(rnorm(100,9,89))
hist(rnorm(100,9,2))
hist(myNum)
myNum = rnorm(100,9,2)
hist(myNum)
?c
id = 1:200
group = c(rep("Vehicle", 100), rep("Drug", 100))
group = c(rep("Vehicle", 100),
rep("Drug", 100))
response = c(rnorm(100,25,5), rnorm(100,23,5))
?data.frame
myData = data.frame(Patient = id, Treatment = group, Response = response)
myData
myData
myData
head(myData)
head(myData,20)
tail(myData)
dim(myData)
?dim
str(myData)
summary(myData)
as.numeric("Apple", "Orange")
as.numeric("1", "2")
as.numeric(c("1", "2"))
myData[1,2]
myData[1,3]
myData[1:3,]
myData[1]
myData[,1]
myData[1]
myData[1,]
myData[1]
myData[,1]
myData[myData$Response<26,]
myData$CondnTrue = myData$Treatment = "Vehicle"
myData
myData$CondnTrue = myData$Treatment == "Vehicle"
myData
myData$Treatment = "Drug"
myData
getwd()
pokemon = read.csv("pokemon.csv")
pokemon = read.xlsx("Pokemon.csv",1)
library(xlsx)
install.packages("xlsx")
library(xlsx)
pokemon = read.xlsx("Pokemon.csv",1)
pokemonnew = read.xlsx("Pokemon.csv",1)
wb <- createWorkbook()
getwd()
pokemonnew = read.xlsx("Pokemon.csv",1)
library(xlsx)
pokemonnew = read.xlsx("Pokemon.csv", sheetIndex = 1)
setwd("~/MIT Analytics Edge/Week 4")
rm(list=ls())
stevens = read.csv("stevens.csv")
table(stevens$LowerCourt, stevens$Reverse)
library(caTools)
set.seed(3000)
split = sample.split(stevens$Reverse, SplitRatio = 0.7)
Train = subset(stevens, split == T)
Test = subset(stevens, split == F)
install.packages("rpart")
library(rpart)
install.packages("rpart.plot")
library(rpart.plot)
#Build model
StevensTree = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, method = "class", minbucket = 25)
prp(StevensTree)
PredictCart = predict(StevensTree, newdata = Test, type ="class")
PredictCart
table(Test$Reverse, PredictCart)
library(ROCR)
PredictROC = predict(StevensTree, newdata = Test)
PredictROC
pred = prediction(PredictROC[,2], Test$Reverse)
perf = performance(pred,"tpr","fpr")
plot(perf)
as.numeric(performance(pred, "auc")@y.values)
StevensTree1 = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, method = "class", minbucket = 5)
prp(StevensTree1)
prp(StevensTree)
prp(StevensTree1)
StevensTree2 = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, method = "class", minbucket = 100)
prp(StevensTree2)
prp(StevensTree1)
install.packages(randomForest)
install.packages("randomForest")
library("randomForest")
StevensForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, nodesize = 25, ntree = 200)
Train$Reverse = as.factor(Train$Reverse)
Test$Reverse = as.factor(Test$Reverse)
StevensForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, nodesize = 25, ntree = 200)
PredictForest = predict(StevensForest, newdata = Test)
table(Test$Reverse, PredictForest) # Confusion matrix
StevensForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, nodesize = 25, ntree = 200)
Train$Reverse = as.factor(Train$Reverse)
Test$Reverse = as.factor(Test$Reverse)
StevensForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, nodesize = 25, ntree = 200)
PredictForest = predict(StevensForest, newdata = Test)
table(Test$Reverse, PredictForest) # Confusion matrix
PredictForest = predict(StevensForest, newdata = Test)
table(Test$Reverse, PredictForest) # Confusion matrix
StevensForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, nodesize = 25, ntree = 200)
PredictForest = predict(StevensForest, newdata = Test)
table(Test$Reverse, PredictForest) # Confusion matrix
set.seed(100)
StevensForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, nodesize = 25, ntree = 200)
PredictForest = predict(StevensForest, newdata = Test)
table(Test$Reverse, PredictForest) # Confusion matrix
(46+74)/(46+74+19+31)
set.seed(200)
StevensForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, nodesize = 25, ntree = 200)
PredictForest = predict(StevensForest, newdata = Test)
table(Test$Reverse, PredictForest) # Confusion matrix
(43+75)/(43+75+18+34)
install.packages("caret")
library(caret)
install.packages("e1071")
library(e1071)
numFolds = trainControl(method="cv", number=10)
cpGrid = expand.grid(.cp = seq(0.01,0.5,0.01))
train(Reverse~Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, method="rpart",trControl = numFolds, tuneGrid = cpGrid)
StevensTreeCV = rpart(Reverse~Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, data = Train, method = "class", cp=0.18)
StevensTreeCV = rpart(Reverse~Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, method = "class", cp=0.18)
PredictCV = predict(StevensTreeCV, newdata = Test, type = "class")
table(Test$Reverse, PredictCV)
prp(StevensTreeCV)
rm(list=ls())
boston = read.csv("boston.csv")
str(boston)
plot(boston$LAT, boston$LON)
points(boston$LAT[boston$CHAS ==1], boston$LON[boston$CHAS==1], col="blue",pch=19)
points(boston$LAT[boston$TRACT ==3531], boston$LON[boston$TRACT ==3531], col="blue",pch=19)
points(boston$LAT[boston$TRACT ==3531], boston$LON[boston$TRACT ==3531], col="red",pch=19)
points(boston$LAT[boston$NOX > 0.55], boston$LON[boston$NOX > 0.55], col="green",pch=19)
summary(boston$NOX)
plot(boston$LAT, boston$LON)
summary(boston$MEDV)
points(boston$LAT[boston$MEDV > 21.2], boston$LON[boston$NOX > 21.2], col="red",pch=19)
points(boston$LAT[boston$MEDV > 21.2], boston$LON[boston$NOX > 21.2], col="red",pch=19)
points(boston$LAT[boston$MEDV > 21.2], boston$LON[boston$MEDV > 21.2], col="red",pch=19)
plot(boston$LON, boston$LAT)
points(boston$LON[boston$MEDV > 21.2], boston$LAT[boston$MEDV > 21.2], col="red",pch=19)
latlonlm = lm(MEDV ~ LAT + LON, data=boston)
summary(latlonlm)
library(rpart)
library(rpart.plot)
latlontree = rpart(MEDV  ~ LAT + LON, data=boston)
prp(latlontree)
plot(boston$LON, boston$LAT)
points(boston$LON[boston$MEDV > 21.2], boston$LAT[boston$MEDV > 21.2], col="red",pch=19)
fittedvalues = predict(latlontree)
points(boston$LON[fittedvalues > 21.2], boston$LAT[fittedvalues > 21.2], col="red",pch=19)
points(boston$LON[fittedvalues > 21.2], boston$LAT[fittedvalues > 21.2], col="blue",pch="$")
latlontree = rpart(MEDV  ~ LAT + LON, data=boston, minbucket=50)
plot(latlontree)
text(latlontree)
plot(boston$LON, boston$LAT)
points(boston$LON[boston$MEDV > 21.2], boston$LAT[boston$MEDV > 21.2], col="red",pch=19)
points(boston$LON[fittedvalues > 21.2], boston$LAT[fittedvalues > 21.2], col="blue",pch="$")
plot(latlontree)
text(latlontree)
plot(boston$LON, boston$LAT)
abline(v=-71.07)
plot(latlontree)
text(latlontree)
plot(boston$LON, boston$LAT)
abline(v=-71.07)
abline(h=42.17)
abline(h=42.28)
points(boston$LON[boston$MEDV > 21.2], boston$LAT[boston$MEDV > 21.2], col="red",pch=19)
library(caTools)
set.seed(123)
split = sample.split(boston$MEDV, SplitRatio = 0.7)
Train = subset(boston, split == T)
Test = subset(boston, split == F)
linreg = lm(MEDV ~ LAT + LON + CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO, data=Train)
summary(linreg)
lingreg.pred = predict(linreg,newdata = Test)
lingreg.sse = sum((lingreg.pred - Test$MEDV)^2)
lingreg.sse
tree = rpart(MEDV ~ LAT + LON + CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO, data=Train)
prp(tree)
tree = rpart(MEDV ~ LAT + LON + CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO, data=Train, minbucket = 5)
prp(tree)
tree = rpart(MEDV ~ LAT + LON + CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO, data=Train, minbucket = 50)
prp(tree)
tree = rpart(MEDV ~ LAT + LON + CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO, data=Train)
prp(tree)
tree.pred = predict(tree, newdata =Test)
tree.sse = sum((tree.pred - Test$MEDV)^2)
tree.sse
install.packages("caret")
install.packages("caret")
install.packages("e1071")
library(caret)
library(e1071)
tr.control = trainControl(method = "cv", number = 10)
cp.grid = expand.grid(.cp = (0:10)*0.001)
tr= train(MEDV ~ LAT + LON + CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO, data=Train, method = "rpart",trControl = tr.control, tuneGrid = cp.grid)
tr
cp.grid = expand.grid( .cp = (0:10)*0.001)
tr = train(MEDV ~ LAT + LON + CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO, data = Train, method = "rpart", trControl = tr.control, tuneGrid = cp.grid)
tr
best.tree = tr$finalModel
prp(best.tree)
library(rpart.plot)
prp(best.tree)
best.tree.pred = predict(best.tree, newdata=Test)
best.tree.sse = sum((best.tree.pred - Test$MEDV)^2)
best.tree.sse
rm(list=ls())
data(state)
statedata = data.frame(state.x77)
str(statedata)
summary(statedata)
head(statedata)
LMModel = lm(Life.Exp ~ ., data=statedata)
summary(LMModel)
LMPredict = predict(LMModel, data=statedata)
sum((LMPredict - statedata$Life.Exp)^2)
LMModel2 = lm(Life.Exp ~ Population + Murder + Frost + HS.Grad, data=statedata)
summary(LMModel2)
LMPredict2 = predict(LMModel2, data=statedata)
sum((LMPredict2 - statedata$Life.Exp)^2)
cor(statedata)
library(rpart)
library(rpart.plot)
TreeModel = rpart(Life.Exp ~ ., data=statedata)
summary(TreeModel)
prp(TreeModel)
TreePredict = predict(TreeModel, data=statedata)
sum((TreePredict - statedata$Life.Exp)^2)
TreePredict1 = predict(TreeModel, data=statedata, minbucket =5)
prp(TreePredict1)
TreePredict1 = predict(TreeModel, data=statedata, minbucket =5)
prp(TreePredict1)
TreePredict1 = rpart(TreeModel, data=statedata, minbucket =5)
TreeModel1 = rpart(Life.Exp ~ ., data=statedata, minbucket = 5)
prp(TreeModel1)
TreePredict1 = predict(TreeModel1, data=statedata)
sum((TreePredict1 - statedata$Life.Exp)^2)
TreeModel2 = rpart(Life.Exp ~ Area, data=statedata, minbucket = 1)
prp(TreeModel2)
TreePredict2 = predict(TreeModel2, data=statedata)
sum((TreePredict2 - statedata$Life.Exp)^2)
library(caret)
library(e1071)
set.seed(111)
tr.control = trainControl(method = "cv", number = 10)
rm(list=ls())
setwd("~/MIT Analytics Edge/Week 4")
letters = read.csv("letters_ABPR.csv")
str(letters_ABPR)
str(letters)
summary(letters)
letters$isB = as.factor(letters$letter == "B")
str(letters)
library(caTools)
set.seed(1000)
split = sample.split(letters$isB, SplitRatio = 0.5)
Train = subset(letters, split == TRUE)
Test = subset(letters, split=F)
Test = subset(letters, split==F)
#Baseline model -- Not B
table(letters$isB)
2350/( 2350+766)
#Baseline model -- Not B
table(Test$isB)
1175/(1175+383) # Accuracy = 0.754172
library(rpart)
library(rpart.plot)
CARTb = rpart(isB ~ . - letter, data= Train, method = "class")
prp(CARTb)
PredictB = predict(CARTb, newdata = Test, type = "class")
table(Test$isB, PredictB)
(340 + 1118)/(240+1118+57+43) # Accuracy =
(340 + 1118)/(340+1118+57+43) # Accuracy =
library("randomForest")
ForestB = randomForest(isB ~ . - letter, data= Train)
set.seed(1000)
ForestB = randomForest(isB ~ . - letter, data= Train)
PredictForestB = predict(ForestB, newdata = Test)
table(Test$isB, PredictForestB)
(1163 + 374)/(1163 + 374 + 12 + 9)
letters$letter = as.factor( letters$letter )
set.seed(2000)
split1 = sample.split(letters$letter, SplitRatio = 0.5)
TrainNew = subset(letters, split1 == T)
TestNew = subset(letters, split1 == F)
table(letters$letter)
table(TestNew$letter) # baseline prediction is the most common occurrence - P
401/nrow(TestNew) # Accuracy =
CARTLetter = rpart(letter ~ . - isB, data = Train, method = "class")
prp(CARTLetter)
CARTLetter = rpart(letter ~ . - isB, data = TrainNew, method = "class")
prp(CARTLetter)
CARTLetter = rpart(letter ~ . - isB, data = TrainNew, method = "class")
prp(CARTLetter)
CARTLetterPredict = predict(CARTLetter, newdata = TestNew, type = "class")
table(TestNew$letter, CARTLetterPredict)
(348 + 318 + 363 + 340)/nrow(TestNew) # Accuracy =
library(randomForest)
ForestLetter = randomForest(letter ~ . - isB, data = TrainNew)
ForestPredict = predict(ForestLetter, newdata = TestNew)
table(TestNew$letter, ForestPredict)
(390 + 380 + 394 + 361)/nrow(TestNew)
rm(list = ls())
setwd("~/MIT Analytics Edge/Week 4")
census = read.csv("census.csv")
library(caTools)
set.seed(2000)
split = sample.split(census$over50k, SplitRatio = 0.6)
train = subset(census, split == T)
test = subset(census, split == F)
LogModel = glm(over50k ~ ., data = train, family = binomial)
summary(LogModel)
PredictLog = predict(LogModel, newdata = test, type = "response")
PredictLog
table(test$over50k, PredictLog > 0.5)
(9051 + 1888)/nrow(test) # Accuracy =
# Baseline model for test set
table(test$over50k)
3078/nrow(test)
9713/nrow(test)
library(ROCR)
ROCRPred = prediction(PredictLog, test$over50k)
as.numeric(performance(ROCRpred,"auc")@y.values)
as.numeric(performance(ROCRPred,"auc")@y.values)
rm(list=ls())
